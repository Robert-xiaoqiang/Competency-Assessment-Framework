NAME: 'bert4qa-cl-trainer-hotpot-tmp'
DEVICE: 'cuda:0,1,2,3'
SEED: 32767
SUMMARY_DIR: '/home/xqwang/projects/qgqa/qabase/summary'
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
MODEL:
  LKEY: bert
  SUBKEY: 'bert-base-uncased'
CURRICULUM:
  ORDER_FILENAME: '/home/xqwang/projects/qgqa/evaluation/hotpot_training/difficulty/sentences_linguistic_reasoning.json'
  START_PERCENT: 0.04
  INCREASE_INTERVAL: 1000
  INCREASE_FACTOR: 2
TRAIN:
  TRAINER: CLTrainer
  DATAPROCESSOR: CLDataProcessor
  PROCESSOR: HotpotProcessor
  CONVERTER: HotpotConverter
  DATASET_VERSION: 2
  DATASET_FILENAME: '/home/xqwang/projects/qgqa/hotpot/hotpot_train_v1.1.json'
  MAX_INPUT_LENGTH: 384
  DOC_STRIDE: 128
  MAX_QUERY_LENGTH: 64
  BATCH_SIZE: 8
  WORKERS: 8
  NUM_EPOCHS: 2 # just for fair comparison with supervised baselines
  NUM_WARMUP_STEPS: 0
  RESUME: True
  LOSS_FREQ: 10
  TB_FREQ: 10
  DEV_FREQ: 50
  LR: 0.00005
  WD: 0.0005
  REDUCTION: mean
  DO_LOWER_CASE: True
DEV:
  DEDUCER: Deducer
  PROCESSOR: HotpotProcessor
  CONVERTER: HotpotConverter
  DATASET_FILENAME: '/home/xqwang/projects/qgqa/hotpot/hotpot_dev_distractor_v1_toy.json'
  MAX_INPUT_LENGTH: 384
  DOC_STRIDE: 128
  MAX_QUERY_LENGTH: 64
  BATCH_SIZE: 8
  WORKERS: 8
  SNAPSHOT_KEY: latest
  
  N_BEST: 20
  DO_LOWER_CASE: True
  MAX_ANSWER_LENGTH: 50
  NULL_SCORE_DIFF_THRESHOLD: 0.0